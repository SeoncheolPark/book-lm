# Asymptotic Theory for Least Squares

기본적인 내용은 @Hansen2022 를 따른다.

::: {#thm-rss}
## Random sampling assumption 

The variables $(Y_i, X_i)$ are a **random sample** if they are mutually independent and identically distributed (i.i.d.) across $i=1,\ldots, n$.
:::

::: {#thm-blp}
## Best linear predictor 관련 assumption 

1.  $E[Y^2]<\infty$
2.  $E\|X\|^2 < \infty$
3.  $Q_{XX} = E[XX^T]$ is positive definite

이 가정의 처음 두 개는 $X$, $Y$가 유한한 평균과 분산, 공분산을 갖음을 의미한다. 세 번째는 $Q_{XX}$의 column들이 linearly independent하고 역행렬이 존재함을 보장한다.

($Q_{XX}$가 positive definite일 때 linearly independence는 찾아볼 것)

```{=html}
<!--
positive definiteness와 linearly independent:
https://sites.calvin.edu/scofield/courses/m355/handouts/definiteMatrices.pdf
-->
```
:::

위의 random sampling과 finite second moment assumption을 가져간채로 least squares estimation에 대한 assumption을 다시 정리한다. (@Hansen2022 Assumption 7.1)

1.  The variables $(Y_i, X_i)$, $i=1,\ldots, n$ are i.i.d.
2.  $E[Y^2]<\infty$.
3.  $E\|X\|^2<\infty$.
4.  $Q_{XX} = E[XX^T]$ is positive definite.

## Consistency of Least Squares Estimator

이 절의 목표는 $\hat{\beta}$가 $\beta$에 consistent함을

1.  weak law of large numbers (WLLN)
2.  continuous mapping theorem (CMT)

을 이용해 보이는 것이다. (@Hansen2022 7.2)

Derivation을 다음과 같은 요소들로 구성된다.

1.  OLS estimatior가 sample moment들의 집합의 연속함수로 표현될 수 있다.
2.  WLLN을 이용해 sample moments가 population moments에 converge in probability함을 보인다.
3.  CMT를 이용해 연속함수에서 converges in probability가 보존됨을 보장한다

그렇다면 먼저 OLS estimator를 다음과 같이 sample moments $\hat{Q}_{XX}=\frac{1}{n}\sum_{i=1}^{n} X_i X_i^{T}$와 $\hat{Q}_{XX}=\frac{1}{n}\sum_{i=1}^{n} X_i Y_i$의 함수로 쓸 수 있다.

$$
\hat{\beta} = \Big(\frac{1}{n}\sum_{i=1}^n X_i X_i^T \Big)^{-1} \Big(\frac{1}{n}\sum_{i=1}^n X_i Y_i \Big) = \hat{Q}_{XX}^{-1}\hat{Q}_{XY}
$$

$(Y_i, X_i)$가 mutually i.i.d. 라는 가정은 $(Y_i, X_i)$로 구성된, 예를 들면 $X_i X_i^{T}$와 $X_i Y_i$가 i.i.d.임을 의미한다. 이들은 또한 앞선 Assumption 7.1에 의해 finite expectation을 갖는다. 이러한 조건 하에서, $n\rightarrow \infty$일 때 WLLN은

$$
\hat{Q}_{XX} = \frac{1}{n}\sum_{i=1}^n X_i X_i^{T} \stackrel{p}{\rightarrow} E[XX^T] = Q_{XX}, \quad{} \hat{Q}_{XY} = \frac{1}{n}\sum_{i=1}^n X_i Y_i\stackrel{p}{\rightarrow} E[XY] = Q_{XY}.
$$ {#eq-Hansen0701}

그 다음 continuous mapping theorem을 써서 $\hat{\beta} \rightarrow \beta$임을 보일 수 있다는 것이다. $n\rightarrow \infty$일 때,

$$
\hat{\beta} = \hat{Q}_{XX} ^{-1}\hat{Q}_{XY} \stackrel{p}{\rightarrow}Q_{XX}^{-1}Q_{XY} = \beta.
$$

Stochastic order notation으로 다음과 같이 쓸 수 있다.

$$
\hat{\beta} = \beta + o_p (1).
$$ {#eq-Hansen0704}

## Asymptotic Normality

Asymptotic normality를 다룰 때에는

1.  먼저 estimator를 sample moment의 함수로 쓰는 것으로부터 시작한다.
2.  그리고 그것들 중 하나가 zero-mean random vector의 sum으로 표현될 수 있고 이는 CLT를 적용 가능케 한다.

우선 $\hat{\beta}- \beta = \hat{Q}_{XX}^{-1}\hat{Q}_{Xe}$라고 두자. 그리고 이를 $\sqrt{n}$에 곱하면 다음 표현을 얻을 수 있다.

$$
\sqrt{n}(\hat{\beta} - \beta) = \Big( \frac{1}{n}\sum_{i=1}^n X_i X_i^T \Big)^{-1} \Big(\frac{1}{\sqrt{n} }\sum_{i=1}^n X_i e_i \Big).
$$ {#eq-Hansen0705}

즉 normalized and centered estimator $\sqrt{n}(\hat{\beta} - \beta)$는 (1) sample average의 함수 $\Big( \frac{1}{n}\sum_{i=1}^n X_i X_i^T \Big)^{-1}$과 normalized sample average $\Big(\frac{1}{\sqrt{n} }\sum_{i=1}^n X_i e_i \Big)$의 곱으로 쓸 수 있다.

그러면 뒷부분은 $E[Xe]=0$이고 이것의 $k\times k$ 공분산함수를 다음과 같이 둘 수 있다. $$
\Omega = E[(Xe)(Xe)^T] = E[XX^T e^2].
$$ 그리고 아래 가정에서처럼 $\Omega <\infty$라는 가정 하에 $X_i e_i$는 i.i.d. mean zero, 유한한 분산을 갖고 CLT에 의해 $$
\frac{1}{\sqrt{n}} \sum_{i=1}^n X_i e_i \stackrel{d}{\rightarrow}\mathcal{N}(0,\Omega).
$$

(@Hansen2022 Assumption 7.2)

1.  The variables $(Y_i, X_i), i=1,\ldots, n$ are i.i.d.
2.  $E[Y^4]<\infty$.
3.  $E\|X\|^4 <\infty$.
4.  $Q_{XX} = E[XX^{T}]$ is positive definite.

여기서 두 번째 조건이 $\Omega <\infty$임을 의미한다. $\Omega <\infty$임을 보이려면 $jl$번째 원소 $E[X_jX_le^2]$이 유한함을 보이면 될 것이다. Properties of Linear Projection Model (@Hansen2022 Theorem 2.9.6) (If $E|Y|^r <\infty$ and $E|X|^r <\infty$ for $r\geq 2$, then $E|e|^r <\infty$)을 이용해 위의 2, 3번 조건에 의해 $E[e^4]<\infty$임을 보일 수 있다. 그러면 expectation inequality에 의해 $\Omega$의 $jl$번째 원소는 다음과 같이 bounded된다. $$
|E[X_jX_le^2]|\leq E|X_jX_le^2| = E[|X_j||X_l|e^2].
$$ Cauchy-Schwarz 부등식을 적용하면 다음과 같다. $$
(E[X_j^2 X_\ell^2])^{1/2} (E[e^4])^{1/2} \leq (E[X_j^4])^{1/4}(E[X_{\ell}^4])^{1/4}(E[e^4])^{1/2}<\infty.
$$

::: {#thm-finitecov}
앞선 가정은 $$
\Omega <\infty
$$ 를 내포하고 $$
\frac{1}{\sqrt{n}} \sum_{i=1}^n X_i e_i \stackrel{d}{\rightarrow} \mathcal{N}(0,\Omega)
$$ {#eq-Hansen0707} as $n\rightarrow \infty$.
:::

식 @eq-Hansen0701 , @eq-Hansen0705 , @eq-Hansen0707 을 함께 쓰면 다음과 같다. $$
\sqrt{n}(\hat{\beta}-\beta) \stackrel{d}{\rightarrow} \mathbf{Q}_{XX}^{-1}\mathcal{N}(0,\Omega) = \mathcal{N}(0, \mathbf{Q}_{XX}^{-1}\Omega \mathbf{Q}_{XX}^{-1}) \quad{} \text{as } n\rightarrow\infty. 
$$ 여기서 마지막 등식은 normal vector의 linear combination이 normal이라는 것에서부터 왔다.

::: {#thm-asymnormalls}
## Asymptotic normality of least squares estimator

앞선 가정 하에서, $n\rightarrow\infty$일 때 $$
\sqrt{n}(\hat{\beta} - \beta) \stackrel{d}{\rightarrow}\mathcal{N}(0, \mathbf{V}_{\beta})
$$ where $\mathbf{Q}_{XX} = E[XX^T]$, $\Omega = E[XX^T e^2]$, and $$
\mathbf{V}_{\beta} = \mathbf{Q}_{XX}^{-1}\Omega \mathbf{Q}_{XX}^{-1}.
$$
:::

Stochastic order notation으로 다음과 같이 쓸 수 있다. $$
\hat{\beta} = \beta + O_p(n^{-1/2}).
$$ 이는 식 @eq-Hansen0704 보다 더 강한 조건이라고 한다.

::: callout-tip
## Remark

-   원래 $o_p$가 더 강한조건이긴 하나 order의 차이가 나서 저렇게 말하는 듯
:::

-   $\mathbf{V}_{\beta}$: **asymptotic covariance matrix** of $\hat{\beta}$, $$
    \mathbf{V}_{\beta} = \mathbf{Q}_{XX}^{-1}\Omega \mathbf{Q}_{XX}^{-1}
    $$ 이며 $\sqrt{n}(\hat{\beta}-\beta)$의 asymptotic distribution의 variance

::: callout-tip
## Remark

-   $\mathbf{V}_{\beta}$의 형태를 보면 $\Omega$가 $\mathbf{Q}_{XX}^{-1}$ 사이에 끼어있는 형태이므로 **sandwich** form이라고 부름
:::
