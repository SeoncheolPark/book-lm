% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
  \setmainfont[]{Noto Sans KR}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\newcounter{none} % for unnumbered tables
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother


% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}



\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\usepackage{makeidx}
\makeindex
\usepackage{titling}
\usepackage{pdfpages}
\usepackage{atbegshi}% http://ctan.org/pkg/atbegshi
\usepackage{kotex}
\let\oldmaketitle\maketitle
\AtBeginDocument{\let\maketitle\relax}
\AtBeginDocument{\AtBeginShipoutNext{\AtBeginShipoutDiscard}} % Discard next blank page





\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{plain}
\newtheorem{corollary}{Corollary}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\AtBeginDocument{\renewcommand*{\proofname}{Proof}}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\newtheorem{refremark}{Remark}[chapter]
\newtheorem{refsolution}{Solution}[chapter]
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{algorithm}{}{\usepackage{algorithm}}
\makeatother
\makeatletter
\@ifpackageloaded{algpseudocode}{}{\usepackage{algpseudocode}}
\makeatother
\makeatletter
\@ifpackageloaded{fontspec}{}{\usepackage{fontspec}}
\makeatother
\makeatletter
\@ifpackageloaded{draftwatermark}{}{\usepackage{draftwatermark}}
\makeatother
\makeatletter
\@ifpackageloaded{xcolor}{}{\usepackage{xcolor}}
\makeatother
\makeatletter
\@ifpackageloaded{forloop}{}{\usepackage{forloop}}
\makeatother
    \definecolor{watermark}{HTML}{000000}
    
    \newcounter{watermarkrow}
    \newcounter{watermarkcol}

    \DraftwatermarkOptions{
      text={
        \begin{tabular}{c}
          \forloop{watermarkrow}{0}{\value{watermarkrow} < 50}{
            \forloop{watermarkcol}{0}{\value{watermarkcol} < 10}{
              { Seoncheol\ Park}\hspace{4.000000em}
            }
            \\[4.000000em]
          }
        \end{tabular}
      },
      fontsize=1.000000em,
      angle=15.000000,
      color=watermark!10
    }
    
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={A Biggner's Guide to Linear Models},
  pdfauthor={Seoncheol Park},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{A Biggner's Guide to Linear Models}
\author{Seoncheol Park}
\date{2026-02-10}
\begin{document}
\maketitle

\renewcommand{\Return}{\State \textbf{return}~}
\newcommand{\Print}{\State \textbf{print}~}
\newcommand{\Break}{\State \textbf{break}}
\newcommand{\Continue}{\State \textbf{continue}}
\newcommand{\True}{\textbf{true}}
\newcommand{\False}{\textbf{false}}
\renewcommand{\And}{\textbf{and}~}
\newcommand{\Or}{\textbf{or}~}
\renewcommand{\Not}{\textbf{not}~}
\newcommand{\To}{\textbf{to}~}
\newcommand{\DownTo}{\textbf{downto}~}

\floatname{algorithm}{Algorithm}

\renewcommand*\contentsname{Table of contents}
{
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

열심히 공부합시다.

\part{Intro}

\chapter{Introduction}\label{introduction}

\begin{itemize}
\item
  \textbf{회귀분석(regression analysis)}: 설명변수와 반응변수 사이의
  함수관계를 알아내는 통계적 방법
\item
  용어의 역사: Galton의
  \href{https://en.wikipedia.org/wiki/Regression_toward_the_mean}{\texttt{Regression\ toward\ the\ mean}}란
  말에서부터 유래함
\end{itemize}

\section{Galton's data}\label{galtons-data}

\textbf{Q}. 아버지와 아들 사이의 키 상관관계?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(HistData)}
\NormalTok{xx }\OtherTok{=}\NormalTok{ GaltonFamilies}\SpecialCharTok{$}\NormalTok{midparentHeight}
\NormalTok{yy }\OtherTok{=}\NormalTok{ GaltonFamilies}\SpecialCharTok{$}\NormalTok{childHeight}

\FunctionTok{plot}\NormalTok{(xx, yy, }\AttributeTok{xlab=}\StringTok{"Father"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Child"}\NormalTok{, }\AttributeTok{main=}\StringTok{"Galton\textquotesingle{}s data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio,alt={Figure: Galton's dataset}]{intro_files/figure-pdf/unnamed-chunk-1-1.pdf}

}

\caption{Figure: Galton's dataset}

\end{figure}%

\part{Simple Linear Regression}

\chapter{Simple Linear Regression}\label{simple-linear-regression-1}

\section{Regression analysis}\label{regression-analysis}

\begin{itemize}
\item
  \textbf{Goal}: Find a linear relationship between an explanatory
  variable (\(X\)) and a response variable \((Y)\)
\item
  \textbf{Assumptions}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Linearity \[
  E(Y|X=x) = \beta_0 + \beta_1 x
  \]
\item
  \(Y|x\) follows a normal distribution
\item
  Constant variance \[
  \text{Var}(Y|X=x)=\sigma^2 <\infty
  \]
\item
  Explanatory variable \(X\) is a fixed variable (not random)
\item
  Response variable \(Y\) is a random variable with measurement error
  \(\varepsilon \sim (0, \sigma^2)\)
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \textbf{Simple linear regression model}
\end{itemize}

\[
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i \quad{} \text{or}\quad{} Y=\beta_0 + \beta_1 X +\varepsilon
\]

\section{Ordinary least squares (OLS)}\label{ordinary-least-squares-ols}

With \(n\) data points \((x_i, y_i)_{i=1}^n\), our goal is to find the
\textbf{best} linear fit of the data \[
(x_i, \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i)_{i=1}^n
\]

\textbf{Q}. What is the \textbf{best} fit?

\href{https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss}{Gauss}가
제안한 방식은 다음의 \textbf{ordinary least squares (OLS)}이다. \[
(\hat{\beta}_0, \hat{\beta}_1) = \arg\min_{\beta_0, \beta_1} \frac{1}{n}\sum_{i=1}^n (y_i - \beta_0 -\beta_1 x_i)^2
\]

\begin{exercise}[Least absolute deviation
(LAD)]\protect\hypertarget{exr-OLS01}{}\label{exr-OLS01}

\textbf{Least absolute deviation (LAD)}에 대해 조사해보자.

\end{exercise}

위의 식을 풀기 위해 각각을 \(\beta_0, \beta_1\)로 미분 후 \(0\)이 되는
\(\hat{\beta}_0, \hat{\beta}_1\)을 찾는 전략을 이용하게 되는데, 여기서
\textbf{정규방정식(normal equation)}을 얻게 된다. \[
\begin{cases}
-\frac{2}{n}\sum_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) &=0\\
-\frac{2}{n}\sum_{i=1}^n x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) &=0
\end{cases}
\]

\part{Nonlinear and Nonparametric Models}

\chapter{Boosting}\label{boosting}

\section{Boosting: 개요}\label{boosting-uxac1cuxc694}

\begin{itemize}
\item
  Boosting의 가장 큰 특징: base learner를 sequentially하게 fitting함
\item
  Base learner로는 weak learner를 사용: tree를 예로 들면 한 번 정도
  split한 tree를 base learner로 사용
\end{itemize}

https://www.uio.no/studier/emner/matnat/math/STK-IN4300/h22/slides/lect10\_modified.pdf

\section{AdaBoost}\label{adaboost}

\section{Gradient boosting}\label{gradient-boosting}

부스팅 공부할 만한 자료:
\url{https://mlcourse.ai/book/topic10/topic10_gradient_boosting.html}

\subsection{\texorpdfstring{\(L^2\)
boosting}{L\^{}2 boosting}}\label{l2-boosting}

\begin{itemize}
\tightlist
\item
  Reference:
  \url{https://mdporter.github.io/DS6030/lectures/boosting.pdf}
\end{itemize}

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio,alt={L2 boosting}]{boosting_files/figure-pdf/unnamed-chunk-1-1.pdf}

}

\caption{L2 boosting}

\end{figure}%

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio,alt={L2 boosting}]{boosting_files/figure-pdf/unnamed-chunk-1-2.pdf}

}

\caption{L2 boosting}

\end{figure}%

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio,alt={L2 boosting}]{boosting_files/figure-pdf/unnamed-chunk-1-3.pdf}

}

\caption{L2 boosting}

\end{figure}%

\subsection{Distributional gradient
boosting}\label{distributional-gradient-boosting}

\begin{itemize}
\tightlist
\item
  \href{https://ar5iv.labs.arxiv.org/html/2204.00778}{Distirbutional
  gradient boosting}
\end{itemize}

\chapter{커널회귀}\label{uxcee4uxb110uxd68cuxadc0}

\section{RKHS}\label{rkhs}

어떤 \(n\times p\) 행렬 \(A\)가 있을 때 이것의 column space를
\(C(A)\)라고 하자.

Ronald Christensen은 아래 \(C(XX^T)=C(X)\)의 결과를 \textbf{the
fundamental theorem of reproducing kernel Hilbert spaces}라고 부른다.

\begin{definition}[Two column spaces are
equiv]\protect\hypertarget{def-equivcs}{}\label{def-equivcs}

For any matrix \(X\), \(C(XX^T)=C(X)\).

\end{definition}

\begin{proof}
Clearly \(C(XX^T)\subset C(X)\), so we need to show that
\(C(X) \subset C(XX^T)\). Let \(x \in C(X)\). Then \(x = Xb\) for some
\(b\). Write \(b=b_0 + b_1\), where \(b_0 \in C(X^T)\) and
\(b1 \perp C(X^T)\). Clearly, \(Xb_1 = 0\), so we have \(x=Xb_0\). But
\(b_0=X^Td\) for some \(d\); so \(x=Xb_0=XX^Td\) and \(x\in C(XX^T)\).
\end{proof}

\begin{definition}[Equivalent Linear
Models]\protect\hypertarget{def-equivlm}{}\label{def-equivlm}

If \(Y=X_1 \beta_1 + e_1\) and \(Y = X_2 \beta_2 +e_2\) are two models
for the same dependent variable vector \(Y\), the models are
\textbf{equivalent} if \(C(X_1) = C(X_2)\).

Since \(C(X) = C(XX^T)\), this implies that the linear models
\(Y= X\beta_1 +e_1\) and \(Y=XX^T\beta_2 +e_2\) are equivalent.

\end{definition}

RKHS는 \(p\)-벡터 \(x_i\)를 \(s\)-벡터 \(\phi_i\)로
\(\phi_i =\begin{bmatrix} \phi_0 (x_i), \cdots \phi_{s-1}(x_i) \end{bmatrix}^T\)로
변환시킨다. \(X\)를 \(x_i^T\)들이 행으로 구성된 행렬로 보면 똑같은
논리로 \(\phi_i^T\)가 행으로 구성된 행렬 \(\Phi\)를 생각할 수 있다.
\(X^X=[x_i^Tx_j]\)를 \(x_i\)들의 inner products로 만드는 \(n\times n\)
행렬로 보면 RKHS는 \textbf{reproducing kernel} \(R(\cdot, \cdot)\)이
존재해 \[
\tilde{R} \equiv [ R(x_i, x_j)]= [\phi_i^T D(\eta) \phi_j] = \Phi D(\eta) \Phi^T
\] 가 \(\phi_i\)들의 \(n\times n\) inner product matrix이며
\(D(\eta)\)가 positive definite diagonal matrix가 됨을 말해준다.
\(D(\eta)\)가 positive definite diagonal matrix이므로 PA책 Theorem
B.22에 의해 \(D(\eta)=QQ^T\)인 정방행렬 \(Q\)가 존재할 것이고 the
fundamental theorem of reproducing kernel Hilbert spaces에 따라 \(s\)가
유한하면 \(C[\Phi D(\eta) \Phi^T] = C(\Phi)\)일 것이다. 따라서 rk 모형
\[
Y = \tilde{R}\gamma + e
\] 를 적합하는 것은 다음의 비모수모형 \[
Y = \Phi \beta + e
\] 를 적합하는 것과 같다. 즉 rk 모형은 \(\beta=D(\eta) \Phi^T \gamma\)로
reparametrization한 것이다. 특별히 rk 모형을 이요해 예측하는 것은 다음과
같이 하면 된다. \[
\hat{y}(x) = \begin{bmatrix} R(x,x_1), \ldots, R(x,x_n) \end{bmatrix} \hat{\gamma}.
\]

\(\Phi\)를 가지고 linear structure를 적합하는 것이나 \(n \times n\) 행렬
\(\tilde{R}\)을 이용해 적합하는 것이나 같을 것이고 이를 \textbf{kernel
trick}이라 한다.

\begin{theorem}[Hilbert space가 RKHS가 되기 위한
조건]\protect\hypertarget{thm-hilbertRKHS}{}\label{thm-hilbertRKHS}

A Hilbert space is a RKHS iff the evaluation functionals are continuous.

\end{theorem}

\section{Kernel Trick}\label{kernel-trick}

Kernel trick의 가장 큰 장점은 알려진 함수 \(R(\cdot, \cdot)\)을 쓰므로
\(\tilde{R}\)을 만들어내기 쉽다는 것이다. 반대로 \(\phi_j (\cdot)\)
함수들에서 \(s\)를 specify하는 것은 시간이 더 걸릴 것이다.

또한 \(n\times s\)행렬 \(\Phi\)는 \(s\)가 크면 이상해지는데,
\(\tilde{R}\)은 항상 \(n\times n\)이 되어 \(s\)가 너무 커질때
이상해지거나 \(s\)가 너무 작을때 단순화되는 것을 막아준다.

\(s\geq n\)이고 \(x_i\)들이 distinct (같은 값을 갖는 \(x\)들이 없다는
뜻)라면 \(\tilde{R}\)은 \(n \times n\) 이고 rank \(n\)인 행렬이며 이것은
saturated model (데이터 수 만큼 모수가 있는 모형)을 만든다. LS
estimate는 fitted value가 obs와 같은 자료를 만들 것이며 d.f는 0이 될
것이다. 즉 overfitting이 있는 것인데, 그래서 보통 kernel trick은
penalized (regularized) estimation과 같이 사용하게 된다.

\(s\geq n\)일 때에는 다른 \(R(\cdot, \cdot)\)을 선택한다 하더라도, 같은
\(C(\tilde{R})\)을 주어 같은 모형을 주는 셈이 된다. 즉 같은 least
squares fits를 준다. 그러나 parametrization을 다르게 하고 거기에
penalty를 주는 방식 (ridge, LASSO 등)으로 다른 fitted value를 만들어낼
수 있다.

사용하려고 하는 \(\phi_j\) 함수들을 다 알고 있을 경우, rk를 쓰는 이득이
었다. 그러나 \(\phi_j\)를 다루기 어렵거나 \(s=\infty\)일 경우에는 rks가
도움이 될 것이다.

다음은 많이 쓰이는 rks들을 정리해 놓았다. \(\|u-v\|\)에만 의존하는
rk들을 \textbf{radial basis function} rk라고 부른다.

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Names
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(R(u,v)\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Polynomial of degree \(d\) & \((1+u^Tv)^d\) \\
Polynomial of degree \(d\) & \(b(c+u^Tv)^d\) \\
Gaussian (radial basis) & \(\exp (-b\|u-v\|^2)\) \\
Sigmoid (hyperbolic tangent) & \(\text{tanh}(bu^Tv +c)\) \\
Linear spline (\(u,v\) scalars) & \(\min (u,v)\) \\
Cubic spline (\(u,v\) scalars) &
\(\max (u,v) \min^2 (u,v)/2 - \min^3(u,v)/6\) \\
Thin plate spline (2 dimensions) & \(\|u -v\|^2 \log (\|u - v\|)\) \\
\end{longtable}
}

표에 있는 것들 중 hyperbolic tangent는 \(\tilde{R}\)을 not nonnegatively
definite한 것들로 줄 수도 있어 실제로 rk는 아니다. 그러나 \(u\)에 대해서
연속인 어떤 \(R(u,v)\)든지 \[
f(x) = \sum_{j=1}^n \gamma_j R(x,x_j)
\] 와 같은 형태의 모형 적합을 유도해낼 수 있기 때문에 이러한 것들을 쓰는
것도 설득력이 있다.

Rk의 아이디어는 함수를 small support를 이용해 근사하는 방법들, 즉
1차원에서 \(s_{*}\)개의 집합으로 나누고 line의 partition을 만들어내는
wavelet, B-spline 등의 방법과 비교할 수 있다. 이러한 방법들은 당연히
\(p\)차원에서 \(s_{*}^p\)개의 dimension을 생각해야 하고 고차원에서
다루기 어렵게 된다. 그러나 kernel method에서는 각 data point에 커널을
적합하는 셈이므로 \(p\)가 크게 커져도 괜찮다.

\section{Kernel Trick과 SVM}\label{kernel-trickuxacfc-svm}

함수 안에 dot product가 있으면 kernel trick을 쓸 수 있다고 한다. 이러한
것들 중 대표적인 것이 SVM이다. SVM의 objective function은 다음과 같다.
\[
\max_{\alpha} \sum_i \alpha_i - \frac{1}{2}\sum_i \sum_j \alpha_i \alpha_j y_i y_j \pmb{x}_i^T\pmb{x}_j ,\quad{} \text{s.t. } \sum_{i} \alpha_i y_i = 0.
\] 이 objective 함수 안에는 dot product \(\pmb{x}_i^T\pmb{x}_j\)가 들어
있고 kernel trick을 쓸 수 있어 SVM이 강력해진다.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{images/kernel_ktrick01.png}}

}

\caption{\label{fig-kernelreg01}Figure: Example of a labeled data
inseparable in 2-Dim is separable in 3-Dim.}

\end{figure}%

위 그림에서, 원래 자료 \(\pmb{x} = \{x_1, x_2\}\)는 2차원에 있는데
이것은 inseparable 하지만 변환 \[
\Phi (\pmb{x}) \rightarrow (x_1^2, x_2^2, \sqrt{2}x_1x_2)
\] 를 이용해 오른쪽 그림과 같이 바꾸면 분리가능하다.

앞선 \(\Phi\) 변환을 이용했을 때 3D 공간에서 decision boundary는 다음과
같다. \[
\beta_0 + \beta_1 x_1^2 + \beta_2 x_2^2 + \beta_3 \sqrt{2} x_1 x_2 = 0.
\] 만약 로지스틱과 같은 회귀를 이용한다면 위의 식과 같은 모형을 쓸
것이다. 그러나 SVM에서는 kernel trick을 이용해 decision boundary를 만들
수 있다. 이를 위해
\(\langle \Phi(\pmb{x}_i), \Phi(\pmb{x}_j) \rangle\)의 dot product를
찾아야 한다.

\begin{align*}
\langle \Phi(\pmb{x}_i), \Phi(\pmb{x}_j) \rangle &= \langle \{x_{i1}^2, x_{i2}^2, \sqrt{2}x_{i1}x_{i2}\}, \{x_{i1}^2, x_{i2}^2, \sqrt{2}x_{i1}x_{i2}\} \rangle\\
&= x_{i1}^2 + x_{j1}^2 + x_{i2}^2 x_{j2}^2 + 2x_{i1}x_{i2}x_{j1}x_{j2}
\end{align*}

일단 이것을 하려면

\begin{itemize}
\tightlist
\item
  \(\Phi\)를 정의해야 하고
\item
  \(\Phi\) 변환 계산시 \(3\times 2\)의 계산
\item
  그리고 dot product를 계산하는 데 3번 해서
\end{itemize}

총 9번의 계산이 필요하다. 그러나 만약 커널
\(K(\pmb{x}_i,\pmb{x}_j)=\langle \pmb{x}_i,\pmb{x}_j\rangle^2\)을
이용한다면, 변환 \(\Phi\)를 찾을 필요도 없고, 그냥 2차원 공간에서 바로
고차원의 similarity measure (dot product)를 만들어낼 수 있다.

\begin{align*}
\langle \pmb{x}_i,\pmb{x}_j\rangle^2 &= \langle\{ x_{i1}, x_{i2}\},\{ x_{i1}, x_{i2}\} \rangle^2\\
&= (x_{i1}x_{j1} + x_{i2}x_{j2})^2\\
&= x_{i1}^2 + x_{j1}^2 + x_{i2}^2 x_{j2}^2 + 2x_{i1}x_{i2}x_{j1}x_{j2}
\end{align*}

로 만들어낼 수 있다. 이것을 하려면

\begin{itemize}
\tightlist
\item
  \(K\)는 정의해야 하나
\item
  두 번째 식까지 두 번의 operation
\item
  마지막 식에서 제곱을 위해 한 번의 operation
\end{itemize}

3번의 계산이 필요하다.

다른 예제를 보자. Decision boundary를 다음과 같이 정하였다. \[
\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1^2 + \beta_4 x_2^2 + \beta_5 \sqrt{2} x_1 x_2 = 0.
\]

\(\Phi(\pmb{x}) \rightarrow (1, \sqrt{2}x_1, \sqrt{2}x_2, x_1^2, x_2^2, \sqrt{2}x_1 x_2)\)와
같이 5차원 변환 \(\Phi\)를 이용해 구하는 방법은 총 16번의 계산을 필요로
한다.

\begin{align*}
\langle \Phi(\pmb{x}_i), \Phi(\pmb{x}_j) \rangle &= \langle \{1, \sqrt{2}x_{i1}, \sqrt{2}x_{i2}, x_{i1}^2, x_{i2}^2, \sqrt{2}x_{i1}x_{i2}\}, \{1, \sqrt{2}x_{i1}, \sqrt{2}x_{i2}, x_{i1}^2, x_{i2}^2, \sqrt{2}x_{i1}x_{i2}\} \rangle\\
&= 1+ 2x_{i1}x_{j1} + 2x_{i2}x_{j2} +x_{i1}^2 x_{j1}^2 + x_{i2}^2 x_{j2}^2 +  x_{i2}^2 x_{j2}^2 + 2x_{i1}x_{i2}x_{j1}x_{j2}
\end{align*}

그러나 커널
\(K(\pmb{x}_i,\pmb{x}_j)=\langle 1+ \langle\pmb{x}_i,\pmb{x}_j\rangle\rangle^2\)를
이용하면 세 번의 계산으로 된다고 한다. \begin{align*}
\langle 1+ \langle\pmb{x}_i,\pmb{x}_j\rangle\rangle^2 &= (1+ x_{i1}x_{j1}+x_{i2}x_{j2})^2\\
&=1+ 2x_{i1}x_{j1} + 2x_{i2}x_{j2} +x_{i1}^2 x_{j1}^2 + x_{i2}^2 x_{j2}^2 +  x_{i2}^2 x_{j2}^2 + 2x_{i1}x_{i2}x_{j1}x_{j2}
\end{align*}

\subsection{무한차원에서의 kernel
trick}\label{uxbb34uxd55cuxcc28uxc6d0uxc5d0uxc11cuxc758-kernel-trick}

앞선 논리를 그대로 적용하면, kernel trick은 infinite space에서도
유사도를 잴 수 있게 해준다. Gaussian Kernel (RBF), exponential kernel,
Laplace kernel 등이 실제로 그러한 역할을 한다. Gaussian kernel은 다음과
같다. \[
K(\pmb{x}_i,\pmb{x}_j)=\exp \Big( - \frac{\| \pmb{x}_i - \pmb{x}_j \|^2}{2\sigma^2} \Big)
\]

\(\sigma=1\)로 두면 위의 Gaussian kernel은
\(C=\exp\Big( -\frac{1}{2}\| \pmb{x}_i\|^2 \Big)\exp\Big( -\frac{1}{2}\| \pmb{x}_j\|^2 \Big)\)으로
두었을 때 \[
\exp \Big( -\frac{1}{2}\| \pmb{x}_i - \pmb{x}_j\|^2 \Big) =C\Big\{ 1- \underbrace{\frac{\langle\pmb{x}_i, \pmb{x}_j \rangle}{1!}}_{\text{1st order}}+\underbrace{\frac{\langle\pmb{x}_i, \pmb{x}_j \rangle^2}{2!}}_{\text{2nd order}}-\underbrace{\frac{\langle\pmb{x}_i, \pmb{x}_j \rangle^3}{3!}}_{\text{3rd order}} + \cdots \Big\}
\] 이러한 표현은 무한차원으로 확장 가능하며, Gaussian kernel이
무한차원에서의 유사도를 찾을 수 있게 해준다.

물론 커널 기반 방법도 커널을 먼저 정해줘야 하며, cross-validation 등을
이용해 커널 함수를 정하거나 또는 커널에 쓰이는 조율모수를 정하게 된다.

\part{Robust and Quantile Regression}

\chapter{Robust Regression}\label{robust-regression}

\section{Robust statistics}\label{robust-statistics}

\textbf{Q}. \(F\)가 오염이 되었을 때 우리가 생각하는 추정량
\(\hat{\theta}\)가 얼마나 영향을 받을 것인가?

\begin{itemize}
\item
  Contaminated distribution function: \[
  F_{\varepsilon} = \varepsilon \delta_y + (1-\varepsilon)F
  \]

  where

  \begin{itemize}
  \item
    \(\delta_y\): mass 1 to the point \(y\)
  \item
    \(F\): a distribution function
  \end{itemize}
\item
  Influence function of \(\hat{\theta}\) at \(F\):

  \[
  IF_{\hat{\theta}} (y, F)=\lim_{\varepsilon\rightarrow 0} \frac{\hat{\theta}(F_{\varepsilon} ) - \hat{\theta}(F)}{\varepsilon}
  \]
\end{itemize}

\section{Robust Regression}\label{robust-regression-1}

\begin{itemize}
\tightlist
\item
  \textbf{Robust regression}: 잡음성이 많은 자료에 사용
\end{itemize}

\chapter{Quantile Regression}\label{quantile-regression}

\section{Check loss function}\label{check-loss-function}

\begin{itemize}
\tightlist
\item
  Check loss function
  \begin{equation}\protect\phantomsection\label{eq-qr-check}{
  \rho_\tau (u) = u(\tau - I(u<0))
  }\end{equation}
\end{itemize}

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio,alt={Figure: Check loss function.}]{quantreg_files/figure-pdf/unnamed-chunk-1-1.pdf}

}

\caption{Figure: Check loss function.}

\end{figure}%

\begin{itemize}
\tightlist
\item
  Loss function: 값이 클수록 손실이 많음

  \begin{itemize}
  \tightlist
  \item
    \(\tau<0.5\): \(\rho_\tau (u)\)는 \(u<0\)일 때 큰 가중치
  \item
    \(\tau>0.5\): \(\rho_\tau (u)\)는 \(u>0\)일 때 큰 가중치
  \end{itemize}
\item
  \(\rho_\tau (u)\)는 \(u=0\)에서 미분 불가능
\end{itemize}

\section{Estimation}\label{estimation}

\begin{itemize}
\item
  Objective function
  \begin{equation}\protect\phantomsection\label{eq-qr-objective}{
  R(\beta) = \sum_{i=1}^n \rho_\tau (y_i - x_i^T \boldsymbol{\beta})=\sum_{i=1}^n \big\{\tau |\varepsilon_i| I[\varepsilon_i \geq 0] + (1-\tau) |\varepsilon_i| I[\varepsilon_i < 0] \big\}
  }\end{equation}
\item
  Directional derivative of \(R\) in direction \(w\): \[
  \begin{aligned}
  \nabla R(\boldsymbol{\beta}, w) &= \frac{d}{dt} R(\boldsymbol{\beta}+ t w) \vert_{t=0}\\
  &=\frac{d}{dt}
  \end{aligned}
  \]
\end{itemize}

\section{Quantile regression as a linear
programming}\label{quantile-regression-as-a-linear-programming}

\begin{itemize}
\item
  Linear program: solving \[
  \min_{\mathbf{z}} \mathbf{c}^T\mathbf{z} \quad{} \text{subject to } A\mathbf{z} = \mathbf{b}, \mathbf{z}\geq 0 
  \]
\item
  선형계획 문제를 풀 때는 \(\mathbf{z}\) 부분에 해당하는 값이 양수여야
  함. 따라서 분위수 회귀문제를 선형계획으로 풀려면 다음과 같이
  \(\varepsilon_i\)를 slack variable을 이용해 양수와 음수 파트로 나눠야
  함. \[
  \varepsilon_i = u_i - v_i
  \]

  이떄

  \begin{itemize}
  \tightlist
  \item
    \(u_i= \max(0,\varepsilon_i) = |\varepsilon_i | I[\varepsilon_i \geq 0]\)
  \item
    \(v_i= \max(0,-\varepsilon_i) = |\varepsilon_i | I[\varepsilon_i < 0]\)
  \end{itemize}
\item
  Then \[
  \sum_{i=1}^n \rho_\tau (\varepsilon_i) = \sum_{i=1}^n \tau u_i + (1-\tau)v_i = \tau \mathbf{1}_n^T \mathbf{u} + (1-\tau)\mathbf{1}_n^T \mathbf{v}
  \] 여기에서 \(\mathbf{u} = (u_1, \ldots, u_n)^T\),
  \(\mathbf{v} = (v_1,\ldots, v_n)^T\)이다.
\item
  Quantile regression objective function
  (Equation~\ref{eq-qr-objective}) may be reformulated as a linear
  program: \[
  \min_{\boldsymbol{\beta},\mathbf{u},\mathbf{v}} \{ \tau \mathbf{1}_n^T \mathbf{u} + (1-\tau)\mathbf{1}_n^T \mathbf{v} \vert \mathbf{y}=\mathbf{X}\boldsymbol{\beta} + \mathbf{u}-\mathbf{v}\}
  \]
\end{itemize}

\begin{itemize}
\item
  알고리즘

  \begin{itemize}
  \item
    Simplex method
  \item
    Frisch-Newton interior point method
  \item
    Sparse regression quantile fitting
  \end{itemize}
\end{itemize}

\subsection{R 코드}\label{r-uxcf54uxb4dc}

\begin{itemize}
\tightlist
\item
  \href{https://stats.stackexchange.com/questions/384909/formulating-quantile-regression-as-linear-programming-problem}{출처:
  stackexchange}
\end{itemize}

\section{Quantile crossing}\label{quantile-crossing}

\begin{itemize}
\item
  분위수 회귀는 기본적으로 각 \(\tau\)에 대해 따로 회귀모형을 적합하는
  방식이기 때문에 낮은 \(\tau\)에서의 조건부 분위수 추정값이 높은
  \(\tau\)에서의 조건부 분위수 추정값보다 높은 역전 현상이 발생할 수도
  있는데, 이를 \textbf{quantile crossing}이라 함
\item
  Quantile crossing을 막으려면 한 번에 conditional distribution 전체를
  모델링 해야 하고, GAMLSS 등이 그런 방법을 취하고 있으나, 분포가정을
  해야 함
\end{itemize}

\part{Linear Model Asymptotics}

\chapter{Asymptotic Theory for Least
Squares}\label{asymptotic-theory-for-least-squares}

기본적인 내용은 Hansen (2022) 를 따른다.

\begin{theorem}[Random sampling assumption (Hansen (2022) Definition
1.2)]\protect\hypertarget{thm-rss}{}\label{thm-rss}

The variables \((Y_i, X_i)\) are a \textbf{random sample} if they are
mutually independent and identically distributed (i.i.d.) across
\(i=1,\ldots, n\).

\end{theorem}

\begin{theorem}[Best linear predictor 관련 assumption (Hansen (2022)
Assumption 2.1)]\protect\hypertarget{thm-blp}{}\label{thm-blp}

~

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(E[Y^2]<\infty\)
\item
  \(E\|X\|^2 < \infty\)
\item
  \(Q_{XX} = E[XX^T]\) is positive definite
\end{enumerate}

이 가정의 처음 두 개는 \(X\), \(Y\)가 유한한 평균과 분산, 공분산을
갖음을 의미한다. 세 번째는 \(Q_{XX}\)의 column들이 linearly
independent하고 역행렬이 존재함을 보장한다.

(\(Q_{XX}\)가 positive definite일 때 linearly independence는 찾아볼 것)

\end{theorem}

위의 random sampling과 finite second moment assumption을 가져간채로
least squares estimation에 대한 assumption을 다시 정리한다. (Hansen
(2022) Assumption 7.1)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The variables \((Y_i, X_i)\), \(i=1,\ldots, n\) are i.i.d.
\item
  \(E[Y^2]<\infty\).
\item
  \(E\|X\|^2<\infty\).
\item
  \(Q_{XX} = E[XX^T]\) is positive definite.
\end{enumerate}

\section{Consistency of Least Squares
Estimator}\label{consistency-of-least-squares-estimator}

이 절의 목표는 \(\hat{\beta}\)가 \(\beta\)에 consistent함을

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  weak law of large numbers (WLLN)
\item
  continuous mapping theorem (CMT)
\end{enumerate}

을 이용해 보이는 것이다. (Hansen (2022) 7.2)

Derivation을 다음과 같은 요소들로 구성된다.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  OLS estimatior가 sample moment들의 집합의 연속함수로 표현될 수 있다.
\item
  WLLN을 이용해 sample moments가 population moments에 converge in
  probability함을 보인다.
\item
  CMT를 이용해 연속함수에서 converges in probability가 보존됨을 보장한다
\end{enumerate}

그렇다면 먼저 OLS estimator를 다음과 같이 sample moments
\(\hat{Q}_{XX}=\frac{1}{n}\sum_{i=1}^{n} X_i X_i^{T}\)와
\(\hat{Q}_{XX}=\frac{1}{n}\sum_{i=1}^{n} X_i Y_i\)의 함수로 쓸 수 있다.

\[
\hat{\beta} = \Big(\frac{1}{n}\sum_{i=1}^n X_i X_i^T \Big)^{-1} \Big(\frac{1}{n}\sum_{i=1}^n X_i Y_i \Big) = \hat{Q}_{XX}^{-1}\hat{Q}_{XY}
\]

\((Y_i, X_i)\)가 mutually i.i.d. 라는 가정은 \((Y_i, X_i)\)로 구성된,
예를 들면 \(X_i X_i^{T}\)와 \(X_i Y_i\)가 i.i.d.임을 의미한다. 이들은
또한 앞선 Assumption 7.1에 의해 finite expectation을 갖는다. 이러한 조건
하에서, \(n\rightarrow \infty\)일 때 WLLN은

\begin{equation}\protect\phantomsection\label{eq-Hansen0701}{
\hat{Q}_{XX} = \frac{1}{n}\sum_{i=1}^n X_i X_i^{T} \stackrel{p}{\rightarrow} E[XX^T] = Q_{XX}, \quad{} \hat{Q}_{XY} = \frac{1}{n}\sum_{i=1}^n X_i Y_i\stackrel{p}{\rightarrow} E[XY] = Q_{XY}.
}\end{equation}

그 다음 continuous mapping theorem을 써서
\(\hat{\beta} \rightarrow \beta\)임을 보일 수 있다는 것이다.
\(n\rightarrow \infty\)일 때,

\[
\hat{\beta} = \hat{Q}_{XX} ^{-1}\hat{Q}_{XY} \stackrel{p}{\rightarrow}Q_{XX}^{-1}Q_{XY} = \beta.
\]

Stochastic order notation으로 다음과 같이 쓸 수 있다.

\begin{equation}\protect\phantomsection\label{eq-Hansen0704}{
\hat{\beta} = \beta + o_p (1).
}\end{equation}

\section{Asymptotic Normality}\label{asymptotic-normality}

Asymptotic normality를 다룰 때에는

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  먼저 estimator를 sample moment의 함수로 쓰는 것으로부터 시작한다.
\item
  그리고 그것들 중 하나가 zero-mean random vector의 sum으로 표현될 수
  있고 이는 CLT를 적용 가능케 한다.
\end{enumerate}

우선 \(\hat{\beta}- \beta = \hat{Q}_{XX}^{-1}\hat{Q}_{Xe}\)라고 두자.
그리고 이를 \(\sqrt{n}\)에 곱하면 다음 표현을 얻을 수 있다.

\begin{equation}\protect\phantomsection\label{eq-Hansen0705}{
\sqrt{n}(\hat{\beta} - \beta) = \Big( \frac{1}{n}\sum_{i=1}^n X_i X_i^T \Big)^{-1} \Big(\frac{1}{\sqrt{n} }\sum_{i=1}^n X_i e_i \Big).
}\end{equation}

즉 normalized and centered estimator \(\sqrt{n}(\hat{\beta} - \beta)\)는
(1) sample average의 함수
\(\Big( \frac{1}{n}\sum_{i=1}^n X_i X_i^T \Big)^{-1}\)과 normalized
sample average \(\Big(\frac{1}{\sqrt{n} }\sum_{i=1}^n X_i e_i \Big)\)의
곱으로 쓸 수 있다.

그러면 뒷부분은 \(E[Xe]=0\)이고 이것의 \(k\times k\) 공분산함수를 다음과
같이 둘 수 있다. \[
\Omega = E[(Xe)(Xe)^T] = E[XX^T e^2].
\] 그리고 아래 가정에서처럼 \(\Omega <\infty\)라는 가정 하에
\(X_i e_i\)는 i.i.d. mean zero, 유한한 분산을 갖고 CLT에 의해 \[
\frac{1}{\sqrt{n}} \sum_{i=1}^n X_i e_i \stackrel{d}{\rightarrow}\mathcal{N}(0,\Omega).
\]

(Hansen (2022) Assumption 7.2)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The variables \((Y_i, X_i), i=1,\ldots, n\) are i.i.d.
\item
  \(E[Y^4]<\infty\).
\item
  \(E\|X\|^4 <\infty\).
\item
  \(Q_{XX} = E[XX^{T}]\) is positive definite.
\end{enumerate}

여기서 두 번째 조건이 \(\Omega <\infty\)임을 의미한다.
\(\Omega <\infty\)임을 보이려면 \(jl\)번째 원소 \(E[X_jX_le^2]\)이
유한함을 보이면 될 것이다. Properties of Linear Projection Model (Hansen
(2022) Theorem 2.9.6) (If \(E|Y|^r <\infty\) and \(E|X|^r <\infty\) for
\(r\geq 2\), then \(E|e|^r <\infty\))을 이용해 위의 2, 3번 조건에 의해
\(E[e^4]<\infty\)임을 보일 수 있다. 그러면 expectation inequality에 의해
\(\Omega\)의 \(jl\)번째 원소는 다음과 같이 bounded된다. \[
|E[X_jX_le^2]|\leq E|X_jX_le^2| = E[|X_j||X_l|e^2].
\] Cauchy-Schwarz 부등식을 적용하면 다음과 같다. \[
(E[X_j^2 X_\ell^2])^{1/2} (E[e^4])^{1/2} \leq (E[X_j^4])^{1/4}(E[X_{\ell}^4])^{1/4}(E[e^4])^{1/2}<\infty.
\]

\begin{theorem}[]\protect\hypertarget{thm-finitecov}{}\label{thm-finitecov}

앞선 가정은 \[
\Omega <\infty
\] 를 내포하고
\begin{equation}\protect\phantomsection\label{eq-Hansen0707}{
\frac{1}{\sqrt{n}} \sum_{i=1}^n X_i e_i \stackrel{d}{\rightarrow} \mathcal{N}(0,\Omega)
}\end{equation} as \(n\rightarrow \infty\).

\end{theorem}

식 Equation~\ref{eq-Hansen0701} , Equation~\ref{eq-Hansen0705} ,
Equation~\ref{eq-Hansen0707} 을 함께 쓰면 다음과 같다. \[
\sqrt{n}(\hat{\beta}-\beta) \stackrel{d}{\rightarrow} \mathbf{Q}_{XX}^{-1}\mathcal{N}(0,\Omega) = \mathcal{N}(0, \mathbf{Q}_{XX}^{-1}\Omega \mathbf{Q}_{XX}^{-1}) \quad{} \text{as } n\rightarrow\infty. 
\] 여기서 마지막 등식은 normal vector의 linear combination이
normal이라는 것에서부터 왔다.

\begin{theorem}[Asymptotic normality of least squares
estimator]\protect\hypertarget{thm-asymnormalls}{}\label{thm-asymnormalls}

앞선 가정 하에서, \(n\rightarrow\infty\)일 때 \[
\sqrt{n}(\hat{\beta} - \beta) \stackrel{d}{\rightarrow}\mathcal{N}(0, \mathbf{V}_{\beta})
\] where \(\mathbf{Q}_{XX} = E[XX^T]\), \(\Omega = E[XX^T e^2]\), and \[
\mathbf{V}_{\beta} = \mathbf{Q}_{XX}^{-1}\Omega \mathbf{Q}_{XX}^{-1}.
\]

\end{theorem}

Stochastic order notation으로 다음과 같이 쓸 수 있다. \[
\hat{\beta} = \beta + O_p(n^{-1/2}).
\] 이는 식 Equation~\ref{eq-Hansen0704} 보다 더 강한 조건이라고 한다.

\begin{tcolorbox}[enhanced jigsaw, breakable, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, titlerule=0mm, left=2mm, toptitle=1mm, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, opacitybacktitle=0.6, toprule=.15mm, colback=white, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Remark}, bottomrule=.15mm, opacityback=0, rightrule=.15mm, bottomtitle=1mm]

\begin{itemize}
\tightlist
\item
  원래 \(o_p\)가 더 강한조건이긴 하나 order의 차이가 나서 저렇게 말하는
  듯
\end{itemize}

\end{tcolorbox}

\begin{itemize}
\tightlist
\item
  \(\mathbf{V}_{\beta}\): \textbf{asymptotic covariance matrix} of
  \(\hat{\beta}\), \[
  \mathbf{V}_{\beta} = \mathbf{Q}_{XX}^{-1}\Omega \mathbf{Q}_{XX}^{-1}
  \] 이며 \(\sqrt{n}(\hat{\beta}-\beta)\)의 asymptotic distribution의
  variance
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, breakable, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, titlerule=0mm, left=2mm, toptitle=1mm, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, opacitybacktitle=0.6, toprule=.15mm, colback=white, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Remark}, bottomrule=.15mm, opacityback=0, rightrule=.15mm, bottomtitle=1mm]

\begin{itemize}
\tightlist
\item
  \(\mathbf{V}_{\beta}\)의 형태를 보면 \(\Omega\)가
  \(\mathbf{Q}_{XX}^{-1}\) 사이에 끼어있는 형태이므로 \textbf{sandwich}
  form이라고 부름
\end{itemize}

\end{tcolorbox}

\chapter{Asymptotic Theory for Quantile
Regression}\label{asymptotic-theory-for-quantile-regression}

\section{Basics}\label{basics}

\textbf{Check function}

\[
\rho_{\tau} (x)= x(\tau - I\{x<0\})=
\begin{cases}
-x (1-\tau), & x<0\\
x\tau, & x \geq 0
\end{cases}.
\]

\[
\psi_{\tau} (x) = \frac{d}{dx}\rho_{\tau}(x) = \tau - I \{x < 0\}, \quad{} x\neq 0.
\]

\begin{theorem}[Consistency of quantile regression
estimator]\protect\hypertarget{thm-consistencyqr}{}\label{thm-consistencyqr}

Assume that \((Y_i, X_i)\) are i.i.d., \(E\vert Y\vert <\infty\),
\(E[\| X\|^2] <\infty\), \(f_{\tau} (e \vert x)\) exists and satisfies
\(f_{\tau}(e\vert x) \leq D < \infty\), and the parameter space for
\(\beta\) is compact. For any \(\tau \in (0,1)\) such that \[
\mathbf{Q}_{\tau} \stackrel{\text{def}}{=} E[XX^T f_{\tau}(0|X)]>0
\] then \(\hat{\beta}_{\tau} \stackrel{p}{\rightarrow} \beta_{\tau}\) as
\(n\rightarrow\infty\).

\end{theorem}

\begin{theorem}[Asymptotic distribution of quantile regression
estimator]\protect\hypertarget{thm-asympdistnqr}{}\label{thm-asympdistnqr}

In addition to the assumptions of Theorem~\ref{thm-consistencyqr} ,
assume that \(f_{\tau}(e\vert x)\) is continuous in \(e\), and
\(\beta_{\tau}\) is in the interior of the parameter space. Then as
\(n\rightarrow \infty\) \[
\sqrt{n}(\hat{\beta}_{\tau} - \beta_{\tau}) \stackrel{d}{\rightarrow} \mathcal{N}(0,\mathbf{V}_{\tau})
\] where
\(\mathbf{V}_{\tau} = \mathbf{Q}_{\tau}^{-1}\Omega_{\tau} \mathbf{Q}_{\tau}^{-1}\)
and \(Q_{\tau} = E[XX^T \psi_{\tau}^2]\) for
\(\psi_{\tau} = \tau - I\{Y<X^T\beta_{\tau}\}\).

\end{theorem}

\part{Advanced Topics}

\chapter{Spatial Linear Models}\label{spatial-linear-models}

\begin{itemize}
\tightlist
\item
  Reference:
  \href{https://www.routledge.com/Spatial-Linear-Models-for-Environmental-Data/Zimmerman-VerHoef/p/book/9780367183349}{Spatial
  Linear Models for Environmental Data}
\end{itemize}

\section{Linear Model}\label{linear-model}

\begin{itemize}
\item
  A \textbf{linear model} for a response variable \(Y\) postulates that
  the response is related to the observed values of \(p\) explanatory
  variables \(X_1, \ldots, X_p\) via this equation: \[
  Y = X_1 \beta_1 + X_2 \beta_2 + \cdots + X_p \beta_p + e,
  \] where
\item
  \(\beta_1, \ldots, \beta_p\): (unknown) parameters
  (\textbf{coefficients})
\item
  \(e\): a random variable (\textbf{error})
\item
  \(X_1 \beta_1 + X_2 \beta_2 + \cdots + X_p \beta_p\): mean structure
\item
  When \(n\) obs \((y_1, \pmb{x}_1), \ldots, (y_, \pmb{x}_n)\) are taken
  on the response and explanatory variables, the linear model as just
  defined implied that \[
  y_i = \pmb{x}_i^T \pmb{\beta} + e_i, \quad{} (i=1,\ldots, n)
  \] where
\item
  \(\pmb{\beta} = (\beta_1, \ldots, \beta_p)^T\) and
  \(e_1, \ldots, e_n\) are \(n\) possibly correlated random variables.
\item
  Using vector and matrix notations: \[
  \pmb{y} = \pmb{X \beta} + \pmb{e}.
  \]
\item
  \(\pmb{e}\): \textbf{error vector}
\item
  \(\pmb{\Sigma}\): \(\text{Var}(\pmb{e})\)
\end{itemize}

\section{Spatial Linear Model}\label{spatial-linear-model}

\begin{itemize}
\tightlist
\item
  A \textbf{spatial linear model} is defined as a linear model for
  spatial data for which the elements of \(\pmb{\Sigma}\) are spatially
  structured functions of the data sites
\end{itemize}

\subsection{Spatial Aitken Model}\label{spatial-aitken-model}

\begin{itemize}
\item
  The simplest spatial linear model is an extension of the Gauss-Markov
  model that we call the \textbf{spatial Aitken model}: \[
  \pmb{\Sigma} = \sigma^2 \pmb{R},
  \] where
\item
  \(\sigma^2\): an unknown positive parameter and
\item
  \(\pmb{R}\) is a \textbf{known} positive definite matrix whose
  elements are spatially structured functions of the data sites. We
  refer to \(\pmb{R}\) as the \textbf{scale-free covariance matrix} of
  the observations; in some settings, but not all, it is a correlation
  matrix.
\item
  Estimation: \textbf{generalized least squares (GLS)}
\end{itemize}

\begin{example}[A toy example of a spatial Aitken
model]\protect\hypertarget{exm-Aitken}{}\label{exm-Aitken}

Consider four obs located at the corners of the unit square in
\(\mathbb{R}^2\), ordered such that the first and last obs are located
at opposite corners of the square. Suppose that the model for the obs is
given by \[
\pmb{R} = 
\begin{pmatrix}
1 & e^{-1} & e^{-1} & e^{-\sqrt{2}}\\
e^{-1} & 1 & e^{-\sqrt{2}} & e^{-1}\\
e^{-1} & e^{-\sqrt{2}} & 1 & e^{-1}\\
e^{-\sqrt{2}} & e^{-1} & e^{-1} & 1
\end{pmatrix}
\]

\end{example}

\begin{itemize}
\item
  \textbf{Symmetric}
\item
  \textbf{Positive definiteness}: the variance of any linear
  combinations of obs having this cov matrix, expect the trivial linear
  combination with all coeffs equal to zero.
\end{itemize}

\section{Spatial General Linear
Model}\label{spatial-general-linear-model}

\begin{itemize}
\item
  A more flexible spatial linear model
\item
  \(\pmb{\Sigma}\) is not fully specified but is given by a
  \textbf{known} spatially structured, matrix-valued parametric function
  \(\pmb{\Sigma}(\pmb{\theta})\), where \(\pmb{\theta}\) is an
  \textbf{unknown} parameter vector.
\item
  Joint parameter space for \(\pmb{\beta}\) and \(\pmb{\theta}\)
  generally taken to be \[
  \{ (\pmb{\beta}, \pmb{\theta}): \pmb{\beta} \in \mathbb{R}^p, \pmb{\theta} \in \Theta \subset \mathbb{R}^m \}
  \] where
\item
  \(\Theta\): the set of vectors \(\pmb{\theta}\) for which
  \(\pmb{\Sigma}(\pmb{\theta})\) is symmetric and positive definite, or
  possibly some subset of that set.
\end{itemize}

\begin{refremark}
A spatial Aitken model is a special case of a spatial general linear
model, with \(\pmb{\theta} \equiv \sigma^2\) and \(\pmb{R}\) not
functionally dependent on any unknown parameters.

\label{rem-Aitken}

\end{refremark}

\begin{example}[A toy example of a spatial Aitken model
(2)]\protect\hypertarget{exm-Aitken2}{}\label{exm-Aitken2}

\[
\pmb{\Sigma}(\pmb{\theta}) = \sigma^2
\begin{pmatrix}
1 & e^{-1/\alpha} & e^{-1/\alpha} & e^{-\sqrt{2}/\alpha}\\
e^{-1/\alpha} & 1 & e^{-\sqrt{2}/\alpha} & e^{-1/\alpha}\\
e^{-1/\alpha} & e^{-\sqrt{2}/\alpha} & 1 & e^{-1/\alpha}\\
e^{-\sqrt{2}/\alpha} & e^{-1/\alpha} & e^{-1/\alpha} & 1
\end{pmatrix}
\]

The parameter space for \(\pmb{\theta} \equiv (\sigma^2, \alpha)^T\)
within which \(\pmb{\Sigma}(\pmb{\theta})\) is p.d. is \[
\{ (\sigma^2, \alpha) : \sigma^2 > 0 , \alpha >0 \}
\]

Small values of \(\alpha\) correspond to weak spatial correlation among
the obs, and as \(\alpha\) increases the spatial correlation among obs
become stronger.

\end{example}

\chapter{Gaussian Processes}\label{gaussian-processes}

\section{Regression analysis}\label{regression-analysis-1}

\begin{itemize}
\item
  All finite collection of realizations (\(n\) obs) is modeled as having
  a multivariate normal (MVN) distribution.
\item
  \textbf{Mean function}: \(\mu (x)\)
\item
  \textbf{Covariance function}: \(\Sigma (x, x')\)
\item
  \(\Sigma (x, x') = \exp \{ - \| x- x' \|^2 \}\)
\item
  \(\Sigma (x, x )=1\)
\item
  \(\Sigma (x, x')\) must be \textbf{positive definite}
\item
  \(\Sigma_n\): covariance matrix (p.d.)
\end{itemize}

\section{Splines vs GP}\label{splines-vs-gp}

\chapter{PCA and Least Squares}\label{pca-and-least-squares}

\section{PCA as least squares
problems}\label{pca-as-least-squares-problems}

PCA can be formulated as follows:

Given \(m\) vectors \(\pmb{x}_1, \ldots, \pmb{x}_m \in \mathbb{R}^n\),
find matrices \(\pmb{U} \in \mathcal{M}_{\mathbb{R}} (k,n)\) and
\(\pmb{V} \in \mathcal{M}_{\mathbb{R}}(n,k)\) such that \[
\sum_{i=1}^m \| \pmb{x}_i - \pmb{V}\pmb{U}\pmb{x}_i\|^2
\] is minimized.

That is, for \(k<n\), the vector \(\pmb{U}\pmb{x}_i \in \mathbb{R}^k\)
is the projection of \(\pmb{x}_i\) into a lower-dimensional subspace,
and \(\pmb{V}\pmb{U}\pmb{x}_i\) is the \textbf{reconstructed} original
vector. PCA aims to find matrices \(\pmb{U}\), \(\pmb{V}\) that minimize
the reconstruction error as measured by the \(\ell^2\)-norm. It can be
shown that, in fact, these matrices are orthogonal and
\(\pmb{U} = \pmb{V}^T\), so the problem reduces to \[
\arg\min_{V\in \mathcal{M}_{\mathbb{R}}(n,k)}\sum_{i=1}^m \| \pmb{x}_i - \pmb{V}\pmb{V}^T \pmb{x}_i \|^2
\] Further manipulations show that \(\pmb{V}\) is the matrix whose
columns are the eigenvectors corresponding to the \(k\) largest
eigenvalues of \[
\sum_{i=1}^m \pmb{x}_i \pmb{x}_i^T
\] as expected. So indeed, PCA is a least squares method and it is quite
sensitive to outliers.

\chapter{Quantile Regression}\label{quantile-regression-1}

\section{More Information}\label{more-information}

You can learn more about creating custom Typst templates here:

\url{https://quarto.org/docs/prerelease/1.4/typst.html\#custom-formats}

\section{Quarto}\label{quarto}

Quarto enables you to weave together content and executable code into a
finished document. To learn more about Quarto see
\url{https://quarto.org}.

\subsection{Running Code}\label{running-code}

When you click the \textbf{Render} button a document will be generated
that includes both content and the output of embedded code. You can
embed code like this:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \SpecialCharTok{+} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2
\end{verbatim}

You can add options to executable code like this

\begin{verbatim}
[1] 4
\end{verbatim}

The \texttt{echo:\ false} option disables the printing of code (only
output is displayed).

\section{Mermaid}\label{mermaid}

\includegraphics[width=6.88in,height=1.81in]{typst_files/figure-latex/mermaid-figure-1.png}

\newpage{}

\begin{theorem}[Line]\protect\hypertarget{thm-line}{}\label{thm-line}

The equation of any straight line, called a linear equation, can be
written as:

\[
y = mx + b
\]

\end{theorem}

\begin{corollary}[Line2]\protect\hypertarget{cor-line2}{}\label{cor-line2}

The equation of any straight line, called a linear equation, can be
written as:

\[
y = mx + b
\]

\end{corollary}

\begin{proof}
가나다라
\end{proof}

\newpage{}

\textbf{?@thm-euclid}

\newpage{}

\chapter{Expectile-based Probabilistic
Forecasting}\label{expectile-based-probabilistic-forecasting}

\chapter{Qunatile-based Probabilistic
Forecasting}\label{qunatile-based-probabilistic-forecasting}

\chapter{Adaptive Boosting on River
Networks}\label{adaptive-boosting-on-river-networks}

\section{Two columns}\label{two-columns}

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod
tempor incididunt utlabore et dolore magnam aliquam quaerat voluptatem.
Ut enim aeque doleamus animo, cumcorpore dolemus, fieri tamen permagna
accessio potest, si aliquod aeternum et infinitumimpendere malum nobis
opinemur. Quod idem licet transferre in voluptatem, ut postea
variarivoluptas distinguique possit, augeri amplificarique non possit.
At etiam Athenis, ut e patreaudiebam facete et urbane Stoicos irridente,
statua est in quo a nobis philosophia defensa etcollaudata est, cum id,
quod maxime placeat, facere possimus, omnis voluptas assumenda est,omnis
dolor repellendus. Temporibus autem quibusdam et aut officiis debitis
aut rerumnecessitatibus saepe eveniet, ut et voluptates repudiandae sint
et molestiae non recusandae.Itaque earum rerum defuturum, quas natura
non depravata desiderat. Et quem ad me accedis,saluto: `chaere,' inquam,
`Tite!' lictores, turma omnis chorusque: `chaere, Tite!' hinc hostis
miAlbucius, hinc inimicus. Sed iure Mucius. Ego autem mirari satis non
queo unde hoc sit taminsolens domesticarum rerum fastidium. Non est
omnino hic docendi locus; sed ita prorsusexistimo, neque eum Torquatum,
qui hoc primus cognomen invenerit, aut torquem illum hostidetraxisse, ut
aliquam ex eo est consecutus? -- Laudem et caritatem, quae sunt vitae.

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod
tempor incididunt utlabore et dolore magnam aliquam quaerat voluptatem.
Ut enim aeque doleamus animo, cumcorpore dolemus, fieri tamen permagna
accessio potest, si aliquod aeternum et infinitumimpendere malum nobis
opinemur. Quod idem licet transferre in voluptatem, ut postea
variarivoluptas distinguique possit, augeri amplificarique non possit.
At etiam Athenis, ut e patreaudiebam facete et urbane Stoicos irridente,
statua est in quo a nobis philosophia defensa etcollaudata est, cum id,
quod maxime placeat, facere possimus, omnis voluptas assumenda est,omnis
dolor repellendus. Temporibus autem quibusdam et aut officiis debitis
aut rerumnecessitatibus saepe eveniet, ut et voluptates repudiandae sint
et molestiae non recusandae.Itaque earum rerum defuturum, quas natura
non depravata desiderat. Et quem ad me accedis,saluto: `chaere,' inquam,
`Tite!' lictores, turma omnis chorusque: `chaere, Tite!' hinc hostis
miAlbucius, hinc inimicus. Sed iure Mucius. Ego autem mirari satis non
queo unde hoc sit taminsolens domesticarum rerum fastidium. Non est
omnino hic docendi locus; sed ita prorsusexistimo, neque eum Torquatum,
qui hoc primus cognomen invenerit, aut torquem illum hostidetraxisse, ut
aliquam ex eo est consecutus? -- Laudem et caritatem, quae sunt vitae.
(\textbf{Azzalini1999?})

\newpage{}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\StringTok{"Hello World!"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Hello World!"
\end{verbatim}

\section{References}\label{references}

\protect\phantomsection\label{refs}
\begin{CSLReferences}{1}{1}
\bibitem[\citeproctext]{ref-Hansen2022}
Hansen, Bruce. 2022. \emph{Econometrics}. Princeton University Press.

\end{CSLReferences}

\bookmarksetup{startatroot}

\chapter{Summary}\label{summary}

In summary, this book has no content whatsoever.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \SpecialCharTok{+} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2
\end{verbatim}

\bookmarksetup{startatroot}

\chapter*{References}\label{references-1}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\protect\phantomsection\label{refs}
\begin{CSLReferences}{1}{1}
\bibitem[\citeproctext]{ref-Hansen2022}
Hansen, Bruce. 2022. \emph{Econometrics}. Princeton University Press.

\end{CSLReferences}




\end{document}
