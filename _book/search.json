[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "book-lm",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "11_asympls.html#introduction",
    "href": "11_asympls.html#introduction",
    "title": "2  Asymptotic Theory for Least Squares",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\n\nTheorem 2.1 (Random sampling assumption (@Hansen2022 Definition 1.2)) The variables \\((Y_i, X_i)\\) are a random sample if they are mutually independent and identically distributed (i.i.d.) across \\(i=1,\\ldots, n\\).\n\n\nTheorem 2.2 (Best linear predictor 관련 assumption (@Hansen2022 Assumption 2.1))  \n\n\\(E[Y^2]&lt;\\infty\\)\n\\(E\\|X\\|^2 &lt; \\infty\\)\n\\(Q_{XX} = E[XX^T]\\) is positive definite\n\n이 가정의 처음 두 개는 \\(X\\), \\(Y\\)가 유한한 평균과 분산, 공분산을 갖음을 의미한다. 세 번째는 \\(Q_{XX}\\)의 column들이 linearly independent하고 역행렬이 존재함을 보장한다.\n(\\(Q_{XX}\\)가 positive definite일 때 linearly independence는 찾아볼 것)\n\n위의 random sampling과 finite second moment assumption을 가져간채로 least squares estimation에 대한 assumption을 다시 정리한다. (@Hansen2022 Assumption 7.1)\n\nThe variables \\((Y_i, X_i)\\), \\(i=1,\\ldots, n\\) are i.i.d.\n\\(E[Y^2]&lt;\\infty\\).\n\\(E\\|X\\|^2&lt;\\infty\\).\n\\(Q_{XX} = E[XX^T]\\) is positive definite."
  },
  {
    "objectID": "11_asympls.html#consistency-of-least-squares-estimator",
    "href": "11_asympls.html#consistency-of-least-squares-estimator",
    "title": "2  Asymptotic Theory for Least Squares",
    "section": "2.2 Consistency of Least Squares Estimator",
    "text": "2.2 Consistency of Least Squares Estimator\n이 절의 목표는 \\(\\hat{\\beta}\\)가 \\(\\beta\\)에 consistent함을\n\nweak law of large numbers (WLLN)\ncontinuous mapping theorem (CMT)\n\n을 이용해 보이는 것이다. (@Hansen2022 7.2)\nDerivation을 다음과 같은 요소들로 구성된다.\n\nOLS estimatior가 sample moment들의 집합의 연속함수로 표현될 수 있다.\nWLLN을 이용해 sample moments가 population moments에 converge in probability함을 보인다.\nCMT를 이용해 연속함수에서 converges in probability가 보존됨을 보장한다\n\n그렇다면 먼저 OLS estimator를 다음과 같이 sample moments \\(\\hat{Q}_{XX}=\\frac{1}{n}\\sum_{i=1}^{n} X_i X_i^{T}\\)와 \\(\\hat{Q}_{XX}=\\frac{1}{n}\\sum_{i=1}^{n} X_i Y_i\\)의 함수로 쓸 수 있다.\n\\[\n\\hat{\\beta} = \\Big(\\frac{1}{n}\\sum_{i=1}^n X_i X_i^T \\Big)^{-1} \\Big(\\frac{1}{n}\\sum_{i=1}^n X_i Y_i \\Big) = \\hat{Q}_{XX}^{-1}\\hat{Q}_{XY}\n\\]\n\\((Y_i, X_i)\\)가 mutually i.i.d. 라는 가정은 \\((Y_i, X_i)\\)로 구성된, 예를 들면 \\(X_i X_i^{T}\\)와 \\(X_i Y_i\\)가 i.i.d.임을 의미한다. 이들은 또한 앞선 Assumption 7.1에 의해 finite expectation을 갖는다. 이러한 조건 하에서, \\(n\\rightarrow \\infty\\)일 때 WLLN은\n\\[\n\\hat{Q}_{XX} = \\frac{1}{n}\\sum_{i=1}^n X_i X_i^{T} \\stackrel{p}{\\rightarrow} E[XX^T] = Q_{XX}, \\quad{} \\hat{Q}_{XY} = \\frac{1}{n}\\sum_{i=1}^n X_i Y_i\\stackrel{p}{\\rightarrow} E[XY] = Q_{XY}.\n\\]\n그 다음 continuous mapping theorem을 써서 \\(\\hat{\\beta} \\rightarrow \\beta\\)임을 보일 수 있다는 것이다. \\(n\\rightarrow \\infty\\)일 때,\n\\[\n\\hat{\\beta} = \\hat{Q}_{XX} ^{-1}\\hat{Q}_{XY} \\stackrel{p}{\\rightarrow}Q_{XX}^{-1}Q_{XY} = \\beta.\n\\]\nStochastic order notation으로 다음과 같이 쓸 수 있다.\n\\[\n\\hat{\\beta} = \\beta + o_p (1).\n\\]"
  },
  {
    "objectID": "11_asympls.html#asymptotic-normality",
    "href": "11_asympls.html#asymptotic-normality",
    "title": "2  Asymptotic Theory for Least Squares",
    "section": "2.3 Asymptotic Normality",
    "text": "2.3 Asymptotic Normality\nAsymptotic normality를 다룰 때에는\n\n먼저 estimator를 sample moment의 함수로 쓰는 것으로부터 시작한다.\n그리고 그것들 중 하나가 zero-mean random vector의 sum으로 표현될 수 있고 이는 CLT를 적용 가능케 한다.\n\n우선 \\(\\hat{\\beta}- \\beta = \\hat{Q}_{XX}^{-1}\\hat{Q}_{Xe}\\)라고 두자. 그리고 이를 \\(\\sqrt{n}\\)에 곱하면 다음 표현을 얻을 수 있다.\n\\[\n\\sqrt{n}(\\hat{\\beta} - \\beta) = \\Big( \\frac{1}{n}\\sum_{i=1}^n X_i X_i^T \\Big)^{-1} \\Big(\\frac{1}{\\sqrt{n} }\\sum_{i=1}^n X_i e_i \\Big).\n\\]즉 normalized and centered estimator \\(\\sqrt{n}(\\hat{\\beta} - \\beta)\\)는 (1) sample average의 함수 \\(\\Big( \\frac{1}{n}\\sum_{i=1}^n X_i X_i^T \\Big)^{-1}\\)과 normalized sample average \\(\\Big(\\frac{1}{\\sqrt{n} }\\sum_{i=1}^n X_i e_i \\Big)\\)의 곱으로 쓸 수 있다.\n그러면 뒷부분은 \\(E[Xe]=0\\)이고 이것의 \\(k\\times k\\) 공분산함수를 다음과 같이 둘 수 있다. \\[\n\\Omega = E[(Xe)(Xe)^T] = E[XX^T e^2].\n\\] 그리고 아래 가정에서처럼 \\(\\Omega &lt;\\infty\\)라는 가정 하에 \\(X_i e_i\\)는 i.i.d. mean zero, 유한한 분산을 갖고 CLT에 의해 \\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n X_i e_i \\stackrel{d}{\\rightarrow}\\mathcal{N}(0,\\Omega).\n\\]\n(@Hansen2022 Assumption 7.2)\n\nThe variables \\((Y_i, X_i), i=1,\\ldots, n\\) are i.i.d.\n\\(E[Y^4]&lt;\\infty\\).\n\\(E\\|X\\|^4 &lt;\\infty\\).\n\\(Q_{XX} = E[XX^{T}]\\) is positive definite.\n\n\\(\\Omega &lt;\\infty\\)임을 보이려면 \\(jl\\)번째 원소 \\(E[X_jX_le^2]\\)이 유한함을 보이면 될 것이다. Properties of Linear Projection Model (@Hansen2022 Theorem 2.9.6) (If \\(E|Y|^r &lt;\\infty\\) and \\(E|X|^r &lt;\\infty\\) for \\(r\\geq 2\\), then \\(E|e|^r &lt;\\infty\\))을 이용해 위의 2, 3번 조건에 의해 \\(E[e^4]&lt;\\infty\\)임을 보일 수 있다. 그러면 expectation inequality에 의해 \\(\\Omega\\)의 \\(jl\\)번째 원소는 다음과 같이 bounded된다. \\[\n|E[X_jX_le^2]|\\leq E|X_jX_le^2| = E[|X_j||X_l|e^2].\n\\]\nStochastic order notation으로 다음과 같이 쓸 수 있다.\n\\[\n\\hat{\\beta} = \\beta + O_p(n^{-1/2}).\n\\]"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  }
]