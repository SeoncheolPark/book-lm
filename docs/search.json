[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Biggner’s Guide to Linear Models",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Galton’s data\nQ. 아버지와 아들 사이의 키 상관관계?\nlibrary(HistData)\nxx = GaltonFamilies$midparentHeight\nyy = GaltonFamilies$childHeight\n\nplot(xx, yy, xlab=\"Father\", ylab=\"Child\", main=\"Galton's data\")\n\n\n\n\nFigure: Galton’s dataset",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02_simplereg.html",
    "href": "02_simplereg.html",
    "title": "2  Simple Linear Regression",
    "section": "",
    "text": "2.1 Regression analysis\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\quad{} \\text{or}\\quad{} Y=\\beta_0 + \\beta_1 X +\\varepsilon\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "02_simplereg.html#regression-analysis",
    "href": "02_simplereg.html#regression-analysis",
    "title": "2  Simple Linear Regression",
    "section": "",
    "text": "Goal: Find a linear relationship between an explanatory variable (\\(X\\)) and a response variable \\((Y)\\)\nAssumptions\n\n\nLinearity \\[\nE(Y|X=x) = \\beta_0 + \\beta_1 x\n\\]\n\\(Y|x\\) follows a normal distribution\nConstant variance \\[\n\\text{Var}(Y|X=x)=\\sigma^2 &lt;\\infty\n\\]\nExplanatory variable \\(X\\) is a fixed variable (not random)\nResponse variable \\(Y\\) is a random variable with measurement error \\(\\varepsilon \\sim (0, \\sigma^2)\\)\n\n\nSimple linear regression model",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "02_simplereg.html#ordinary-least-squares-ols",
    "href": "02_simplereg.html#ordinary-least-squares-ols",
    "title": "2  Simple Linear Regression",
    "section": "2.2 Ordinary least squares (OLS)",
    "text": "2.2 Ordinary least squares (OLS)\n\nWith \\(n\\) data points \\((x_i, y_i)_{i=1}^n\\), our goal is to find the best linear fit of the data \\[\n(x_i, \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)_{i=1}^n\n\\]\nQ. What is the best fit?\nGauss가 제안한 방식은 다음의 ordinary least squares (OLS)이다. \\[\n(\\hat{\\beta}_0, \\hat{\\beta}_1) = \\arg\\min_{\\beta_0, \\beta_1} \\frac{1}{n}\\sum_{i=1}^n (y_i - \\beta_0 -\\beta_1 x_i)^2\n\\]\n\nExercise 2.1 (Least absolute deviation (LAD)) Least absolute deviation (LAD)에 대해 조사해보자.\n\n위의 식을 풀기 위해 각각을 \\(\\beta_0, \\beta_1\\)로 미분 후 \\(0\\)이 되는 \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\)을 찾는 전략을 이용하게 되는데, 여기서 정규방정식(normal equation)을 얻게 된다. \\[\n\\begin{cases}\n-\\frac{2}{n}\\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) &=0\\\\\n-\\frac{2}{n}\\sum_{i=1}^n x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) &=0\n\\end{cases}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "11_asympls.html",
    "href": "11_asympls.html",
    "title": "3  Asymptotic Theory for Least Squares",
    "section": "",
    "text": "3.1 Consistency of Least Squares Estimator\n이 절의 목표는 \\(\\hat{\\beta}\\)가 \\(\\beta\\)에 consistent함을\n을 이용해 보이는 것이다. (Hansen (2022) 7.2)\nDerivation을 다음과 같은 요소들로 구성된다.\n그렇다면 먼저 OLS estimator를 다음과 같이 sample moments \\(\\hat{Q}_{XX}=\\frac{1}{n}\\sum_{i=1}^{n} X_i X_i^{T}\\)와 \\(\\hat{Q}_{XX}=\\frac{1}{n}\\sum_{i=1}^{n} X_i Y_i\\)의 함수로 쓸 수 있다.\n\\[\n\\hat{\\beta} = \\Big(\\frac{1}{n}\\sum_{i=1}^n X_i X_i^T \\Big)^{-1} \\Big(\\frac{1}{n}\\sum_{i=1}^n X_i Y_i \\Big) = \\hat{Q}_{XX}^{-1}\\hat{Q}_{XY}\n\\]\n\\((Y_i, X_i)\\)가 mutually i.i.d. 라는 가정은 \\((Y_i, X_i)\\)로 구성된, 예를 들면 \\(X_i X_i^{T}\\)와 \\(X_i Y_i\\)가 i.i.d.임을 의미한다. 이들은 또한 앞선 Assumption 7.1에 의해 finite expectation을 갖는다. 이러한 조건 하에서, \\(n\\rightarrow \\infty\\)일 때 WLLN은\n\\[\n\\hat{Q}_{XX} = \\frac{1}{n}\\sum_{i=1}^n X_i X_i^{T} \\stackrel{p}{\\rightarrow} E[XX^T] = Q_{XX}, \\quad{} \\hat{Q}_{XY} = \\frac{1}{n}\\sum_{i=1}^n X_i Y_i\\stackrel{p}{\\rightarrow} E[XY] = Q_{XY}.\n\\]\n그 다음 continuous mapping theorem을 써서 \\(\\hat{\\beta} \\rightarrow \\beta\\)임을 보일 수 있다는 것이다. \\(n\\rightarrow \\infty\\)일 때,\n\\[\n\\hat{\\beta} = \\hat{Q}_{XX} ^{-1}\\hat{Q}_{XY} \\stackrel{p}{\\rightarrow}Q_{XX}^{-1}Q_{XY} = \\beta.\n\\]\nStochastic order notation으로 다음과 같이 쓸 수 있다.\n\\[\n\\hat{\\beta} = \\beta + o_p (1).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Asymptotic Theory for Least Squares</span>"
    ]
  },
  {
    "objectID": "11_asympls.html#consistency-of-least-squares-estimator",
    "href": "11_asympls.html#consistency-of-least-squares-estimator",
    "title": "3  Asymptotic Theory for Least Squares",
    "section": "",
    "text": "weak law of large numbers (WLLN)\ncontinuous mapping theorem (CMT)\n\n\n\n\nOLS estimatior가 sample moment들의 집합의 연속함수로 표현될 수 있다.\nWLLN을 이용해 sample moments가 population moments에 converge in probability함을 보인다.\nCMT를 이용해 연속함수에서 converges in probability가 보존됨을 보장한다",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Asymptotic Theory for Least Squares</span>"
    ]
  },
  {
    "objectID": "11_asympls.html#asymptotic-normality",
    "href": "11_asympls.html#asymptotic-normality",
    "title": "3  Asymptotic Theory for Least Squares",
    "section": "3.2 Asymptotic Normality",
    "text": "3.2 Asymptotic Normality\nAsymptotic normality를 다룰 때에는\n\n먼저 estimator를 sample moment의 함수로 쓰는 것으로부터 시작한다.\n그리고 그것들 중 하나가 zero-mean random vector의 sum으로 표현될 수 있고 이는 CLT를 적용 가능케 한다.\n\n우선 \\(\\hat{\\beta}- \\beta = \\hat{Q}_{XX}^{-1}\\hat{Q}_{Xe}\\)라고 두자. 그리고 이를 \\(\\sqrt{n}\\)에 곱하면 다음 표현을 얻을 수 있다.\n\\[\n\\sqrt{n}(\\hat{\\beta} - \\beta) = \\Big( \\frac{1}{n}\\sum_{i=1}^n X_i X_i^T \\Big)^{-1} \\Big(\\frac{1}{\\sqrt{n} }\\sum_{i=1}^n X_i e_i \\Big).\n\\]즉 normalized and centered estimator \\(\\sqrt{n}(\\hat{\\beta} - \\beta)\\)는 (1) sample average의 함수 \\(\\Big( \\frac{1}{n}\\sum_{i=1}^n X_i X_i^T \\Big)^{-1}\\)과 normalized sample average \\(\\Big(\\frac{1}{\\sqrt{n} }\\sum_{i=1}^n X_i e_i \\Big)\\)의 곱으로 쓸 수 있다.\n그러면 뒷부분은 \\(E[Xe]=0\\)이고 이것의 \\(k\\times k\\) 공분산함수를 다음과 같이 둘 수 있다. \\[\n\\Omega = E[(Xe)(Xe)^T] = E[XX^T e^2].\n\\] 그리고 아래 가정에서처럼 \\(\\Omega &lt;\\infty\\)라는 가정 하에 \\(X_i e_i\\)는 i.i.d. mean zero, 유한한 분산을 갖고 CLT에 의해 \\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n X_i e_i \\stackrel{d}{\\rightarrow}\\mathcal{N}(0,\\Omega).\n\\]\n(Hansen (2022) Assumption 7.2)\n\nThe variables \\((Y_i, X_i), i=1,\\ldots, n\\) are i.i.d.\n\\(E[Y^4]&lt;\\infty\\).\n\\(E\\|X\\|^4 &lt;\\infty\\).\n\\(Q_{XX} = E[XX^{T}]\\) is positive definite.\n\n\\(\\Omega &lt;\\infty\\)임을 보이려면 \\(jl\\)번째 원소 \\(E[X_jX_le^2]\\)이 유한함을 보이면 될 것이다. Properties of Linear Projection Model (Hansen (2022) Theorem 2.9.6) (If \\(E|Y|^r &lt;\\infty\\) and \\(E|X|^r &lt;\\infty\\) for \\(r\\geq 2\\), then \\(E|e|^r &lt;\\infty\\))을 이용해 위의 2, 3번 조건에 의해 \\(E[e^4]&lt;\\infty\\)임을 보일 수 있다. 그러면 expectation inequality에 의해 \\(\\Omega\\)의 \\(jl\\)번째 원소는 다음과 같이 bounded된다. \\[\n|E[X_jX_le^2]|\\leq E|X_jX_le^2| = E[|X_j||X_l|e^2].\n\\]\nStochastic order notation으로 다음과 같이 쓸 수 있다.\n\\[\n\\hat{\\beta} = \\beta + O_p(n^{-1/2}).\n\\]\n\n\n\n\nHansen, Bruce. 2022. Econometrics. Princeton University Press.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Asymptotic Theory for Least Squares</span>"
    ]
  },
  {
    "objectID": "21_asympqr.html",
    "href": "21_asympqr.html",
    "title": "4  Asymptotic Theory for Quantile Regression",
    "section": "",
    "text": "4.1 Basics\nCheck function\n\\[\n\\rho_{\\tau} (x)= x(\\tau - I\\{x&lt;0\\})=\n\\begin{cases}\n-x (1-\\tau), & x&lt;0\\\\\nx\\tau, & x \\geq 0\n\\end{cases}.\n\\]\n\\[\n\\psi_{\\tau} (x) = \\frac{d}{dx}\\rho_{\\tau}(x) = \\tau - I \\{x &lt; 0\\}, \\quad{} x\\neq 0.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asymptotic Theory for Quantile Regression</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "5  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Hansen, Bruce. 2022. Econometrics. Princeton University Press.",
    "crumbs": [
      "References"
    ]
  }
]